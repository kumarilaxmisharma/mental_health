# -*- coding: utf-8 -*-
"""Y3T2 - AI and Its Applications (Group Project - Mental Health Prediction).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Hants5yb6rIu9UpWThY6LtTIIHhjMZI

# Mental Health Risk Classification using Machine Learning
## **OBJECTIVE:**
Our GOAL is to develop a web-based AI-powered tool that classifies the mental well-being risk level (`Low Concern`, `Moderate Concern`, `High Concern`) of tech employees based on structured questionnaire responses related to stress, anxiety, burnout, and workplace conditions; to help individuals and companies gain early awareness of mental health risks, encouraging preventive actions and supporting a healthier workplace environment.

## **TECHNIQUES USED:**
Random Forest, Decision Tree, XGBoost

## **MODEL STRATEGY**
We simplify prediction using structured survey data:
- Workplace conditions (support, benefits, interference)
- Mental health history (treatment, family history)
- Demographics (age, gender, country)
- AI model assigns risk category to enable early awareness and support.

## **TEAM**
**F5 Team**
* Kumari Laxmi Sharma
* Thai Sodalin
* Eav Chansotheary
* Sarouen Reachny Ying An
* Nop Puthlinna

## Data Preparation

### Data Collection
**Data Sources:**
Kaggle - [Mental Health in Tech Survey](https://www.kaggle.com/datasets/osmi/mental-health-in-tech-survey)

**Dataset Overview:**
This dataset contains survey responses from `1,259` participants collected in 2014, focusing on attitudes toward mental health and the prevalence of mental health disorders within the technology workplace. It is composed of `27` columns capturing demographic, workplace, and mental health-related information.

**Key Characteristics:**
- Rows: 1,259
- Columns: 27
- Data Types: Mix of numeric (age) and categorical/object (gender, country, mental health status, workplace factors).
"""

import kagglehub
import os
import pandas as pd

# Download the dataset
dataset_path = kagglehub.dataset_download("osmi/mental-health-in-tech-survey")

# Load the file to csv
file_path = dataset_path + '/' + os.listdir(dataset_path)[0]
df = pd.read_csv(file_path)

# Test display data
df.head(3)

"""### Data Understanding & Exploration"""

df



print(df.duplicated().sum()) # check dulplicate data

df.info() # check all columns' name, data types and missing values in the DataFrame

"""### Feature Selection"""

df.drop(['Timestamp','state','comments','Country'], axis=1, inplace=True) # remove irrelevant columns from DataFrame. Keep only meaningful features

"""### Data Cleaning"""

df.info() # check updated columns (after dropping some columns)



# Handle missing value in DataFrame

df[['self_employed', 'work_interfere']] = df[['self_employed', 'work_interfere']].fillna('Unknown') # fill NaN

df.info() # check updated columns (after filling missing values)

# Standardize all the columns' name to 'column_name'
df.columns = [
    'age',
    'gender',
    'self_employed',
    'family_history',
    'treatment',
    'work_interfere',
    'nb_employees',
    'remote_work',
    'tech_company',
    'benefits',
    'care_options',
    'wellness_program',
    'seek_help',
    'anonymity',
    'leave',
    'mental_health_consequence',
    'phys_health_consequence',
    'coworkers',
    'supervisor',
    'mental_health_interview',
    'phys_health_interview',
    'mental_vs_physical',
    'obs_consequence'
]

df.info() # check updated columns' name

"""#### Age Cleaning"""

df['age'].value_counts().sort_index()

# Age Outlier Handling and Imputation
# (In previous cell, 'age' contained invalid values like negatives and extreme numbers (e.g., -1726, -1, 999999), so we need to handle it)
# Here we convert, clean, and impute invalid ages with median to preserve data.

import numpy as np

# Handle outliers in 'age' column
# Convert age to numeric to catch non-numeric entries
df['age'] = pd.to_numeric(df['age'], errors='coerce')

# Define valid age range (assuming working age: 18 to 100)
valid_min_age = 18
valid_max_age = 100

# Mark ages outside the valid range as NaN (treat as outliers)
df.loc[(df['age'] < valid_min_age) | (df['age'] > valid_max_age), 'age'] = np.nan

# Instead of dropping rows with invalid age (since our dataset is small),
# we fill missing ages with the median age to preserve data
median_age = df['age'].median()
df['age'].fillna(median_age, inplace=True)
df['age'] = df['age'].astype(int)

# Check the distribution of cleaned age values
df['age'].value_counts().sort_index()

# Create age groups
bins = [17, 24, 34, 44, 54, 100]
labels = ['18-24', '25-34', '35-44', '45-54', '55+']
df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=True)

# Display the distribution of the new age groups
print("Distribution of Age Groups:")
print(df['age_group'].value_counts().sort_index())

# Drop the original 'age' column as we now have 'age_group'
df.drop('age', axis=1, inplace=True)

# Reorder the columns to make 'age_group' the first column
# Get the list of columns
cols = df.columns.tolist()
# Remove 'age_group' from the list
cols.remove('age_group')
# Insert 'age_group' at the beginning of the list
cols.insert(0, 'age_group')
# Reindex the DataFrame with the new column order
df = df[cols]

"""#### Gender Cleaning"""

df['gender'].value_counts()

# Map all messy gender values to Male, Female and Unknown/Non-Binary
def clean_gender(gender):
    gender = str(gender).strip().lower()

    male_terms = ['male', 'm', 'man', 'cis male', 'cis man', 'msle', 'mal', 'malr', 'make', 'maile', 'mail',
                  'male (cis)', 'male-ish', 'guy (-ish) ^_^', 'something kinda male?', 'ostensibly male, unsure what that really means']

    female_terms = ['female', 'f', 'woman', 'cis female', 'cis-female/femme', 'female (cis)',
                    'femake', 'femail', 'female (trans)', 'trans-female', 'trans woman']

    if gender in male_terms:
        return 'Male'
    elif gender in female_terms:
        return 'Female'
    else:
        return 'Unknown/Non-Binary'

# apply the function to the column Gender
df['gender'] = df['gender'].apply(clean_gender)

# check updated data in column Gender
df['gender'].value_counts()

"""#### Quick Distribution Check
We want to check the data distribution of all columns to ensure our dataset is **clean** and **ready for use**. (since all the remaining columns are from multiple choice questions, so we want to ensure they're all well categorized)
"""

# Check distribution of all columns
for col in df.columns:
    print(f"Value counts for '{col}':")
    print(df[col].value_counts().sort_index())
    print("\n" + "-"*50 + "\n")

"""#### Final Checking"""

df.isnull().sum() # ensure there's no null

df.dtypes # check datatype

"""> Okay, everything's nice!!! Except for the datatype, it's still not quite what we want yet.

> Next step: let's do the  **Data Transformation**.

### Data Transformation

> Why do we do **Data Transformation?**

```
1Ô∏è‚É£ Make Data Usable for Machine Learning Models
- Most ML algorithms (like RandomForest, SVM, Logistic Regression, etc.) only accept numerical input.
- Categorical data (strings like 'Yes', 'Female', 'Remote') must be transformed into numbers.
- Without transformation, models throw errors because they can't process text.

2Ô∏è‚É£ Improve Model Accuracy
- Many transformations help the model understand patterns better.
- Example:
  work_interfere: ['Never', 'Rarely', 'Sometimes', 'Often']
  If left as text ‚Üí model cannot understand the order or importance.
  If encoded properly ‚Üí model can learn better relationships.

3Ô∏è‚É£ Handle Outliers and Inconsistent Data
- Transformations help clean unrealistic or extreme values.
- Ex: Your age column had values like -1726, 99999999999.
- Transformation replaces them with reasonable values (median, mean, etc).

4Ô∏è‚É£ Standardize Feature Scales (when needed)
- Some models (like Logistic Regression, Neural Networks, KNN, etc.) are sensitive to feature scales.
- Normalization or Standardization ensures no feature dominates because of larger numbers.

5Ô∏è‚É£ Memory Optimization (Optional)
- Converting object types to category reduces memory usage.
- Important for very large datasets.

üî¨ In simple words:
üëâ Without transformation ‚Üí ML model = "I don't understand your data"
üëâ With transformation ‚Üí ML model = "Thank you, now I can learn from your data"

üî• One-line summary we can say:
"We do data transformation to convert raw, messy, human-readable data into a machine-readable format, enabling ML models to learn meaningful patterns effectively."
```
"""

# --- Step 2: Standardize and Clean Categorical Data ---
# Unify all variations of unknown answers into a single, consistent 'Unknown' label.
df.replace(["Don't know", "Not sure", "Not applicable"], 'Unknown', inplace=True)

# CRITICAL: Remove leading/trailing whitespace from all object (text) columns.
# This is a common cause of mapping failures.
for col in df.select_dtypes(include=['object']).columns:
    df[col] = df[col].str.strip()

# --- Step 3: Manual Mapping for Ordinal & Specific Features ---
# Use .map() for features with a clear logical or hierarchical order.

# Ordinal features with a defined scale
df['work_interfere'] = df['work_interfere'].map({'Never': 0, 'Rarely': 1, 'Sometimes': 2, 'Often': 3, 'Unknown': -1})
df['leave'] = df['leave'].map({'Very difficult': 0, 'Somewhat difficult': 1, 'Unknown': 2, 'Somewhat easy': 3, 'Very easy': 4})
df['nb_employees'] = df['nb_employees'].map({'1-5': 0, '6-25': 1, '26-100': 2, '100-500': 3, '500-1000': 4, 'More than 1000': 5})
df['age_group'] = df['age_group'].astype(object).map({'55+': 1, '45-54': 2, '35-44': 3, '25-34': 4, '18-24': 5, 'Unknown': -1})

# Features mapping willingness or agreement
df['coworkers'] = df['coworkers'].map({'No': 0, 'Some of them': 1, 'Yes': 2, 'Unknown': -1})
df['supervisor'] = df['supervisor'].map({'No': 0, 'Some of them': 1, 'Yes': 2, 'Unknown': -1})

# Features mapping a scale of concern
df['mental_health_consequence'] = df['mental_health_consequence'].map({'No': 0, 'Maybe': 1, 'Yes': 2, 'Unknown': -1})
df['phys_health_consequence'] = df['phys_health_consequence'].map({'No': 0, 'Maybe': 1, 'Yes': 2, 'Unknown': -1})
df['mental_health_interview'] = df['mental_health_interview'].map({'No': 0, 'Maybe': 1, 'Yes': 2, 'Unknown': -1})
df['phys_health_interview'] = df['phys_health_interview'].map({'No': 0, 'Maybe': 1, 'Yes': 2, 'Unknown': -1})

# --- Step 4: Binary and Yes/No/Unknown Mapping ---
# Map all remaining 'Yes'/'No' based columns to numerical values.

# Columns with 'Yes', 'No', and 'Unknown' options
yes_no_unknown_cols = ['self_employed', 'benefits', 'care_options', 'wellness_program', 'seek_help', 'anonymity', 'mental_vs_physical']
for col in yes_no_unknown_cols:
    if col in df.columns:
         # Convert to object type before mapping and filling NaN
        df[col] = df[col].astype(object).map({'No': 0, 'Yes': 1, 'Unknown': -1})

# Columns with only 'Yes'/'No' answers
binary_cols = ['family_history', 'treatment', 'remote_work', 'tech_company', 'obs_consequence']
for col in binary_cols:
    if col in df.columns:
         # Convert to object type before mapping and filling NaN
        df[col] = df[col].astype(object).map({'No': 0, 'Yes': 1})

# Gender
binary_cols = ['gender']
for col in binary_cols:
    if col in df.columns:
         # Convert to object type before mapping and filling NaN
        df[col] = df[col].astype(object).map({'Female': 0, 'Unknown/Non-Binary':1, 'Male': 2})

# Ensure all columns are of an integer type to avoid floats where not needed.
# This step helps in making the DataFrame uniformly numerical.
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors='coerce').astype('int64')

# Reset the index for a clean final DataFrame.
df.reset_index(drop=True, inplace=True)

# --- Display Final Results ---
print("--- Transformed Data Head ---")
print(df.head())

print("\n--- Final Data Types (All Numerical) ---")
df.info()

print("\n--- Verification of No Boolean Types ---")
print(df.dtypes.value_counts())

# Check distribution of all columns
for col in df.columns:
    print(f"Value counts for '{col}':")
    print(df[col].value_counts().sort_index())
    print("\n" + "-"*50 + "\n")

"""### Rule Base Scoring"""

import pandas as pd
import numpy as np

# Assume 'df' is your pre-processed and numerically encoded DataFrame.
# If you need to recreate it for testing, you can load it from a file or
# use the sample creation from the previous step.

################################################################################
# PART 1: COMPREHENSIVE FEATURE ENGINEERING - CREATING THE RISK LABELS
################################################################################

print("--- PART 1: CREATING MENTAL HEALTH RISK LABELS (COMPREHENSIVE) ---")

def calculate_comprehensive_risk_score(row):
    """
    Calculates a more nuanced mental health risk score by considering both
    risk factors (positive points) and protective factors (negative points).
    """
    score = 0

    # High-Impact Risk Factors (+3)
    if row['treatment'] == 1: score += 3
    if row['work_interfere'] == 3: score += 3

    # Medium-Impact Risk Factors (+2)
    if row['family_history'] == 1: score += 2
    if row['mental_health_consequence'] == 2: score += 2
    if row['work_interfere'] == 2: score += 2

    # Low-Impact Risk Factors (+1)
    if row['self_employed'] == 1: score += 1
    if row['age_group'] == 5: score += 1  # 18-24 age group
    if row['benefits'] in [0, -1]: score += 1
    if row['care_options'] in [0, -1]: score += 1
    if row['wellness_program'] in [0, -1]: score += 1
    if row['seek_help'] in [0, -1]: score += 1
    if row['anonymity'] in [0, -1]: score += 1
    if row['leave'] in [0, 1]: score += 1
    if row['mental_vs_physical'] == 0: score += 1
    if row['coworkers'] == 0: score += 1
    if row['supervisor'] == 0: score += 1
    if row['obs_consequence'] == 1: score += 1

    # Protective Factors (-1)
    if row['benefits'] == 1: score -= 1
    if row['care_options'] == 1: score -= 1
    if row['wellness_program'] == 1: score -= 1
    if row['seek_help'] == 1: score -= 1
    if row['anonymity'] == 1: score -= 1
    if row['leave'] == 4: score -= 1 # Only 'Very easy'
    if row['mental_vs_physical'] == 1: score -= 1

    # Strong Protective Factors (-2)
    if row['supervisor'] == 2: score -= 2

    return score

# --- Step 1.1: Calculate the risk score for each row ---
df['risk_score'] = df.apply(calculate_comprehensive_risk_score, axis=1)

# --- Step 1.2: Categorize scores into risk labels ---
# Use pd.qcut to bin scores into 3 groups: 1 (Low), 2 (Moderate), 3 (High).
# This method is robust because it adapts to the distribution of your new scores.
try:
    df['mental_health_risk'] = pd.qcut(df['risk_score'], q=3, labels=[1, 2, 3])
except ValueError as e:
    print(f"Could not create 3 distinct bins. Error: {e}")
    # This might happen if scores are not varied enough. Let's inspect the scores.
    print("This can happen if scores are not varied enough. Check score distribution.")
    df['mental_health_risk'] = -1 # Assign a default error value

################################################################################
# PART 2: VERIFICATION AND FINAL OUTPUT
################################################################################

print("\n--- PART 2: FINAL RESULTS ---")

print("\n--- Comprehensive Risk Score Distribution ---")
# The describe() output will now show a wider range of scores, including negatives.
print(df['risk_score'].describe())

print("\n--- Value Counts for New Risk Categories ---")
# Check the balance of your new Low, Moderate, and High risk groups.
print(df['mental_health_risk'].value_counts().sort_index())

print("\n--- Final DataFrame Head with Risk Columns ---")
# Displaying key columns to check the logic.
print(df[[
    'treatment', 'supervisor', 'benefits', 'leave',
    'risk_score', 'mental_health_risk'
]].head(20))

"""## Model Development

### Define Target Variable
"""

# Define the feature set (X) and the target variable (y)
# y is our newly created cluster label, which we will use as the target for our supervised models.
y = df['mental_health_risk']

# X contains all the other columns that will be used for prediction.
# We drop the target variable 'mental_health_risk' to create our feature set.
X = df.drop('mental_health_risk', axis=1)

# Display the feature set and target variable to confirm
print("--- Feature Set (X) ---")
print(X.head())
print("\n" + "-"*50 + "\n")
print("--- Target Variable (y) ---")
print(y.head())

"""### Train-Test Split"""

from sklearn.model_selection import train_test_split

# Split data into training and testing sets (80% training, 20% testing)
# test_size=0.2 specifies that 20% of the data will be used for testing.
# random_state=42 ensures that the split is the same every time you run the code, making results reproducible.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting datasets to verify the split
print("Shape of the datasets after splitting:")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""# Train Model"""

# Step 1: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

# Step 6: Define Target and Features
# Target variable was already created as 'mental_health_risk' in previous steps
target = 'mental_health_risk'  # Using the risk classification created earlier
X = df.drop(columns=[target, 'risk_score']) # Dropping the target and the intermediate risk_score
y = df[target]

# Step 7: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y
)

# Convert target variables to integer type
y_train = y_train.astype(int)
y_test = y_test.astype(int)

# Step 8: Train Models with tuned hyperparameters
models = {
    "Random Forest": RandomForestClassifier(
        n_estimators=300,        # more trees improve stability
        max_depth=20,            # limit depth to reduce overfitting
        min_samples_split=5,     # more conservative split
        min_samples_leaf=2,      # prevent very small leaves
        random_state=42
    ),

    "Decision Tree": DecisionTreeClassifier(
        max_depth=10,            # reduce overfitting
        min_samples_split=4,
        min_samples_leaf=2,
        random_state=40
    ),

    "XGBoost": XGBClassifier(
        learning_rate=0.06,
        max_depth=6,
        n_estimators=300,
        subsample=0.8,
        colsample_bytree=0.8,
        gamma=1,
        reg_alpha=0.5,
        reg_lambda=1,
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=41,
    )

}


# Adjust target variable to be zero-indexed for XGBoost
# XGBoost requires target variable to be zero-indexed (0, 1, 2 instead of 1, 2, 3)
y_train_xgb = y_train - 1
y_test_xgb = y_test - 1

results = {}
confusion_matrices = {}

for name, model in models.items():
    print(f"\n--- Training {name} ---")
    if name == 'XGBoost':
        model.fit(X_train, y_train_xgb)
        y_pred = model.predict(X_test)
        cm = confusion_matrix(y_test_xgb, y_pred)
        report = classification_report(y_test_xgb, y_pred)
        accuracy = accuracy_score(y_test_xgb, y_pred)
        precision = precision_score(y_test_xgb, y_pred, average='weighted')
        recall = recall_score(y_test_xgb, y_pred, average='weighted')
        f1 = f1_score(y_test_xgb, y_pred, average='weighted')
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        report = classification_report(y_test, y_pred)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')


    results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'Classification Report': report
    }
    confusion_matrices[name] = cm

    print(f"\n{name} Evaluation:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print("\nClassification Report:")
    print(report)

    # Plot Confusion Matrix
    plt.figure(figsize=(8, 6))
    # Adjust tick labels for XGBoost confusion matrix
    if name == 'XGBoost':
         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2], yticklabels=[0, 1, 2])
    else:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[1, 2, 3], yticklabels=[1, 2, 3]) # Assuming original labels are 1,2,3
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    print("\n" + "="*50 + "\n")


# # Step 9: Visualize Feature Importance (Random Forest Example)
# # Check if Random Forest model exists in models
# if "Random Forest" in models:
#     rf = models["Random Forest"]
#     # Check if the model has feature_importances_ attribute
#     if hasattr(rf, 'feature_importances_'):
#         feat_importances = pd.Series(rf.feature_importances_, index=X.columns)
#         feat_importances.nlargest(15).plot(kind='barh', figsize=(10, 6))
#         plt.title("Top 15 Feature Importances - Random Forest")
#         plt.xlabel("Importance Score")
#         plt.gca().invert_yaxis()
#         plt.tight_layout()
#         plt.show()
#     else:
#         print("\nRandom Forest model does not have feature_importances_ attribute.")
# else:
#     print("\nRandom Forest model not found in the models dictionary.")


# Summarize results
print("--- Model Performance Summary ---")
summary_df = pd.DataFrame(results).transpose()
print(summary_df[['Accuracy', 'Precision', 'Recall', 'F1-Score']])

import matplotlib.pyplot as plt
import seaborn as sns

# Create a countplot to visualize the distribution of mental health risk by gender
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='gender', hue='mental_health_risk', palette='viridis')

# Add titles and labels for better readability
plt.title('Distribution of Mental Health Risk by Gender')
plt.xlabel('Gender')
plt.ylabel('Count')

# Improve x-axis labels for gender
plt.xticks(ticks=[0, 1, 2], labels=['Female', 'Unknown/Non-Binary', 'Male'])

# Add a legend
plt.legend(title='Mental Health Risk', loc='upper right')

# Show the plot
plt.show()

from google.colab import files

files.download('mental_health_model.pkl')